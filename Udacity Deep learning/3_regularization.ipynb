{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 1024\n",
    "L2_weight = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_hidden = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_layer_size]))\n",
    "  biases_hidden = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_layer_output = tf.nn.relu(tf.matmul(tf_train_dataset, weights_hidden)\n",
    "                                   + biases_hidden)\n",
    "  logits = tf.matmul(hidden_layer_output, weights) + biases\n",
    "  l2_loss = L2_weight * (tf.nn.l2_loss(weights_hidden) + tf.nn.l2_loss(weights))\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_hidden)\n",
    "                         + biases_hidden),weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_hidden)\n",
    "                         + biases_hidden), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 757.184143\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 32.6%\n",
      "Minibatch loss at step 500: 201.144745\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1000: 116.563080\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1500: 69.655670\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000: 41.505993\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 2500: 25.015673\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 15.424565\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "hidden_layer_size = 1024\n",
    "L2_weight = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_hidden = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_layer_size]))\n",
    "  biases_hidden = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_layer_output = tf.nn.relu(tf.matmul(tf_train_dataset, weights_hidden)\n",
    "                                   + biases_hidden)\n",
    "  logits = tf.matmul(hidden_layer_output, weights) + biases\n",
    "  l2_loss = 0\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_hidden)\n",
    "                         + biases_hidden),weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_hidden)\n",
    "                         + biases_hidden), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 370.034119\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 25.5%\n",
      "Minibatch loss at step 500: 0.829879\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2000: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Test accuracy: 87.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0]/100 - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_size = 1024\n",
    "L2_weight = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  dropout_p = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Variables.\n",
    "  weights_hidden = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_layer_size]))\n",
    "  biases_hidden = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_layer_output_without_dropout = tf.nn.relu(tf.matmul(tf_train_dataset, weights_hidden)\n",
    "                                                   + biases_hidden)\n",
    "  hidden_layer_output = tf.nn.dropout(hidden_layer_output_without_dropout, dropout_p)\n",
    "  logits = tf.matmul(hidden_layer_output, weights) + biases\n",
    "  l2_loss = L2_weight * (tf.nn.l2_loss(weights_hidden) + tf.nn.l2_loss(weights))\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_hidden)\n",
    "                         + biases_hidden),weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_hidden)\n",
    "                         + biases_hidden), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 796.110596\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 34.1%\n",
      "Minibatch loss at step 500: 202.076553\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1000: 120.055130\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1500: 71.668633\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2000: 41.826447\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2500: 25.041143\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3000: 15.443160\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 91.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_p : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "hidden_layer_1_size = 1024\n",
    "hidden_layer_2_size = 100\n",
    "# hidden_layer_3_size = 128\n",
    "L2_weight = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  dropout_p = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Variables.\n",
    "  weights_hidden_1 = tf.Variable(\n",
    "     tf.truncated_normal([image_size * image_size, hidden_layer_1_size],\n",
    "                          stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "  biases_hidden_1 = tf.Variable(tf.zeros([hidden_layer_1_size]))\n",
    "  weights_hidden_2 = tf.Variable(\n",
    "    tf.truncated_normal([ hidden_layer_1_size, hidden_layer_2_size]))\n",
    "  biases_hidden_2 = tf.Variable(tf.zeros([hidden_layer_2_size]))\n",
    "#   weights_hidden_3 = tf.Variable(\n",
    "#     tf.truncated_normal([hidden_layer_2_size, hidden_layer_3_size]))\n",
    "#   biases_hidden_3 = tf.Variable(tf.zeros([hidden_layer_3_size]))\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layer_2_size, num_labels],\n",
    "                       stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_layer_1_output =\n",
    "    tf.nn.relu(tf.matmul(tf_train_dataset, weights_hidden_1)\n",
    "                                     + biases_hidden_1)\n",
    "  hidden_layer_2_output = \n",
    "        tf.nn.relu(tf.matmul(hidden_layer_1_output, weights_hidden_2) \n",
    "                                     + biases_hidden_2)\n",
    "  hidden_layer_3_output =  tf.nn.dropout(\n",
    "        tf.nn.relu(tf.matmul(hidden_layer_2_output, weights_hidden_3)\n",
    "                                     + biases_hidden_3), 0.5)\n",
    "  logits = tf.matmul(hidden_layer_3_output, weights) + biases\n",
    "  l2_loss = L2_weight * (tf.nn.l2_loss(weights_hidden_1) \n",
    "                         + tf.nn.l2_loss(weights_hidden_2) \n",
    "                         + tf.nn.l2_loss(weights_hidden_3)\n",
    "                         + tf.nn.l2_loss(weights))\n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer\n",
    "  learning_rate = tf.train.exponential_decay(1e-4, global_step, 1500, .9, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, 0.2 * weights_hidden_1) + biases_hidden_1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, 0.2 * weights_hidden_2) + biases_hidden_2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights_hidden_3) + biases_hidden_3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights) + biases)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, 0.2 * weights_hidden_1) + biases_hidden_1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, 0.2 * weights_hidden_2) + biases_hidden_2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights_hidden_3) + biases_hidden_3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 194081.312500\n",
      "Minibatch accuracy: 5.0%\n",
      "Validation accuracy: 11.6%\n",
      "Minibatch loss at step 500: 3920.210693\n",
      "Minibatch accuracy: 26.0%\n",
      "Validation accuracy: 55.9%\n",
      "Minibatch loss at step 1000: 1281.498657\n",
      "Minibatch accuracy: 36.0%\n",
      "Validation accuracy: 52.8%\n",
      "Minibatch loss at step 1500: 1283.926514\n",
      "Minibatch accuracy: 24.0%\n",
      "Validation accuracy: 42.3%\n",
      "Minibatch loss at step 2000: 657.318970\n",
      "Minibatch accuracy: 32.0%\n",
      "Validation accuracy: 39.8%\n",
      "Minibatch loss at step 2500: 581.036743\n",
      "Minibatch accuracy: 24.0%\n",
      "Validation accuracy: 36.8%\n",
      "Minibatch loss at step 3000: 497.393127\n",
      "Minibatch accuracy: 28.0%\n",
      "Validation accuracy: 38.8%\n",
      "Test accuracy: 42.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAFkCAYAAABfHiNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBJIkCQOBJEnCQCBJkjAQSJIk4P8BRbLZkMp0fr4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31cc0d2610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "validation_accuracy = []\n",
    "training_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout_p : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "        training_accuracy.append(accuracy(predictions, batch_labels))\n",
    "        validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  n_steps = range(31)\n",
    "  plt.figure()\n",
    "  plt.plot(n_steps, validation_accuracy, 'b', label='Validation')\n",
    "  plt.plot(n_steps, training_accuracy, 'r', label='Training')\n",
    "  plt.legend(loc = 'upper right')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
